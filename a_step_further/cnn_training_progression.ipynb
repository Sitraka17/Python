{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sitraka17/Python/blob/main/cnn_training_progression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sINgO1_OZzuC"
      },
      "source": [
        "# Oxford AI Summit: Kaggle dataset training notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRfGfZiWZzuC"
      },
      "source": [
        "In this notebook we cover the model development progression when training on the Kaggle fashion product image dataset with various CNN architectures. Initially we used tensorflow to train a basic and advanced CNN from scratch. While we observed a performance imrpovement when including advanced features like batch normalisation and global average pooling, we were still not able to reach desired accuracy statistics. To get around this, we chose to fine tune a pre-trained CNN model called Yolov8. This model has a classification version, along with image segmentation and object detection models and is pre-trained on the ImageNet dataset. We found that performance of this fine-tuned model exceeded that of our tensorflow models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xcdNfJ3ZzuD"
      },
      "source": [
        "## Installation, execution\n",
        "\n",
        "We ran the training for these models in Google Colab environments, where the majority of the pacakages were pre installed, but if you wish to run on a local device you just ensure that libhdf5-dev (or equivalent, depending on your OS) is installed (e.g. sudo apt install libhdf5-dev), along with the following python dependencies:\n",
        "\n",
        "tensorflow\n",
        "pandas\n",
        "kaggle\n",
        "ultralytics\n",
        "scikit-learn\n",
        "\n",
        "Below we only install ultralytics manually as the rest are included in the environment by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NB5UUIMZzuD",
        "outputId": "039b9a1b-91a9-4f5a-b934-b12b398c8e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.28-py3-none-any.whl (779 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.6/779.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=0.2.5 (from ultralytics)\n",
            "  Downloading ultralytics_thop-0.2.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.28 ultralytics-thop-0.2.7\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE5AaWVpZzuD"
      },
      "source": [
        "Note: sometimes the import block needs to be run twice before it suceeded due to some issue with kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXhTh1IOZzuE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import schedules, AdamW\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wnwYRLnZzuE"
      },
      "source": [
        "After importing required pacakges, we must check to see what compute we have available for training. It a TPU is available, then we set it up, otherwise we check to see if there are GPUs available, or if we are operating on CPU alone. Note: this is only required for the tensorflow train, and ultralytics package deals with this separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00sFAxvZzuE",
        "outputId": "9c5fcdd4-4c81-4cf0-faec-de8365edef07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on GPU or CPU\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "{'compute_capability': (7, 5), 'device_name': 'Tesla T4'}\n"
          ]
        }
      ],
      "source": [
        "# Enable TPU if available\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Running on TPU')\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Running on GPU or CPU')\n",
        "    print(tf.config.experimental.list_physical_devices())\n",
        "    if (gpus := tf.config.experimental.list_physical_devices('GPU')):\n",
        "        for gpu in gpus:\n",
        "            print(tf.config.experimental.get_device_details(gpu))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voJjMUwGZzuF"
      },
      "source": [
        "## General Data Preprocessing\n",
        "\n",
        "We then download the fashion product images (small) dataset (only have to run this block once if you are doing multiple trainings/experimenting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZn_a0VxZzuF",
        "outputId": "ef37382a-1494-4499-a4d7-9ad887882e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\n",
            "Downloading fashion-product-images-small.zip to fashion_product_images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0.00/565M [00:00<?, ?B/s]"
          ]
        }
      ],
      "source": [
        "api = KaggleApi()\n",
        "\n",
        "dataset = 'paramaggarwal/fashion-product-images-small'\n",
        "destination_folder = 'fashion_product_images'\n",
        "\n",
        "api.dataset_download_files(dataset, path=destination_folder, unzip=True, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC8Sz8oGZzuF"
      },
      "source": [
        "After loading thre dataset, we must process it to use the article type labels for training, and ensure there is not class mismatch between the training and validation sets (e.g. no socks appearing in validation set could potentially break code). We see there are 30000+ images available for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voduRa74ZzuF"
      },
      "outputs": [],
      "source": [
        "# Load the metadata\n",
        "metadata_path = 'fashion_product_images/myntradataset/styles.csv'\n",
        "metadata = pd.read_csv(metadata_path, on_bad_lines='skip')\n",
        "\n",
        "# Filter the dataset to include only rows with valid images\n",
        "image_folder = 'fashion_product_images/myntradataset/images'\n",
        "metadata['image_path'] = metadata.apply(lambda row: os.path.join(image_folder, str(row['id']) + '.jpg'), axis=1)\n",
        "metadata = metadata[metadata['image_path'].apply(os.path.exists)]\n",
        "\n",
        "# Select relevant columns and convert 'articleType' to category\n",
        "metadata = metadata[['image_path', 'articleType']].copy()\n",
        "metadata['articleType'] = metadata['articleType'].astype('category')\n",
        "metadata['label'] = metadata['articleType'].cat.codes\n",
        "\n",
        "# Ensure each class has at least 2 samples\n",
        "min_samples_per_class = 2\n",
        "class_counts = metadata['label'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
        "metadata = metadata[metadata['label'].isin(valid_classes)]\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_df, val_df = train_test_split(metadata, test_size=0.2, stratify=metadata['label'], random_state=42)\n",
        "\n",
        "# Convert the labels to strings\n",
        "train_df['label'] = train_df['label'].astype(str)\n",
        "val_df['label'] = val_df['label'].astype(str)\n",
        "\n",
        "# Find common classes\n",
        "train_classes = set(train_df['label'].unique())\n",
        "val_classes = set(val_df['label'].unique())\n",
        "common_classes = train_classes.intersection(val_classes)\n",
        "\n",
        "# Filter dataframes to only include common classes\n",
        "train_df = train_df[train_df['label'].isin(common_classes)]\n",
        "val_df = val_df[val_df['label'].isin(common_classes)]\n",
        "\n",
        "# Print the number of unique labels\n",
        "num_classes = len(common_classes)\n",
        "print(f'Number of unique labels: {num_classes}')\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5JmLbGMZzuF"
      },
      "source": [
        "## TensorFlow Training\n",
        "\n",
        "Having preprocessed out data, we then set up training and validation data generators which will load the data on the fly during training (rather than loading all into memort beforehand) and for the training data apply augmentation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J17I6xBLZzuF"
      },
      "outputs": [],
      "source": [
        "# Image data generator with augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Image data generator for validation (without augmentation)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFMg_HbhZzuF"
      },
      "source": [
        "First, we try training a very basic small CNN model with 3 convolutional layers using the Adam optimiser and categorical crossentropy loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku0bTH8WZzuF"
      },
      "outputs": [],
      "source": [
        "# Define a basic CNN model within the strategy scope\n",
        "with strategy.scope():\n",
        "    basic_model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')  # Adjusted number of output units\n",
        "    ])\n",
        "\n",
        "    basic_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    basic_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxTUQQnEZzuG"
      },
      "source": [
        "We also configure an early stopping callback to halt training if the validation accuracy shows no meaningful improvement over the last 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHVFu2u0ZzuG"
      },
      "outputs": [],
      "source": [
        "# Define training callback for early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    min_delta=0,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIg5Kaf4ZzuG"
      },
      "source": [
        "We then train and save the basic model (along with checkpoints per epoch in case the process is interupted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScySPpEgZzuG"
      },
      "outputs": [],
      "source": [
        "# Train the basic model\n",
        "basic_checkpoint = ModelCheckpoint('basic_tf_cp.keras', save_best_only=True)\n",
        "\n",
        "history = basic_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[basic_checkpoint, early_stopping],\n",
        ")\n",
        "basic_model.save('basic_tf.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urNze0j0ZzuG"
      },
      "source": [
        "Then, we try training a larger CNN model with 8 convolutional layers using the AdamW optimiser and categorical crossentropy loss function. This model also incorporates batch normalisation, multiple dropout layers and global average pooling. We also include a learning rate decay schedule and extra metrics to visualise during training (precision, recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgWz5Xt_ZzuG"
      },
      "outputs": [],
      "source": [
        "# Define an advanced CNN model within the strategy scope\n",
        "with strategy.scope():\n",
        "    advanced_model = Sequential([\n",
        "        Conv2D(64, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Conv2D(512, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(512, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(1024, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    advanced_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = schedules.ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=100000,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True\n",
        "    )\n",
        "    advanced_model.compile(\n",
        "        optimizer=AdamW(learning_rate=lr_schedule, weight_decay=0.0001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', 'Precision', 'Recall']\n",
        "    )\n",
        "    advanced_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DOAdZn4ZzuG"
      },
      "source": [
        "We also add an additional callback to reduce the learning rate as the validation loss begins to plateau to help improve training performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-zoTmL2ZzuG"
      },
      "outputs": [],
      "source": [
        "# Define training callback for learning rate reduction\n",
        "lr_reduce = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_TyqbMIZzuG"
      },
      "source": [
        "We then train and save the advanced model (along with checkpoints per epoch in case the process is interupted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snU9ro9HZzuG"
      },
      "outputs": [],
      "source": [
        "# Train the advanced model\n",
        "advanced_checkpoint = ModelCheckpoint('advanced_tf_cp.keras', save_best_only=True)\n",
        "\n",
        "advanced_history = advanced_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[advanced_checkpoint, early_stopping, lr_reduce],\n",
        "    verbose=1,\n",
        ")\n",
        "advanced_model.save('advanced_tf.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PalEOUcHZzuG"
      },
      "source": [
        "# Yolov8 Fine-tuning\n",
        "\n",
        "Having attempted training a model from scratch using tensorflow, we now turn our attention to fine-tuning a pre-trained model to improve performance. We chose the nano version of the Yolov8 classification model. This uses a different method to format the training data to tensorflow so below we include a script to produce the folder structure required, along with the YAML config file for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T0kMsJ1ZzuG"
      },
      "outputs": [],
      "source": [
        "# Function to create directory structure\n",
        "def prepare_dataset(df, base_path):\n",
        "    for _, row in df.iterrows():\n",
        "        class_dir = os.path.join(base_path, row['label'])\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        shutil.copy(row['image_path'], class_dir)\n",
        "\n",
        "\n",
        "data_dir = pathlib.Path(os.getcwd(), 'datasets')\n",
        "train_path = pathlib.Path(data_dir, 'train')\n",
        "val_path = pathlib.Path(data_dir, 'val')\n",
        "\n",
        "# Create directory structure\n",
        "prepare_dataset(train_df, train_path)\n",
        "prepare_dataset(val_df, val_path)\n",
        "\n",
        "# Create the YAML file for the dataset\n",
        "yaml_content = f\"\"\"\n",
        "train: {train_path}\n",
        "val: {val_path}\n",
        "\n",
        "# Number of classes\n",
        "nc: {num_classes}\n",
        "\n",
        "# Class names\n",
        "names:\n",
        "\"\"\"\n",
        "\n",
        "# Add class names to the YAML content\n",
        "class_names = train_df['articleType'].cat.categories\n",
        "for i, class_name in enumerate(class_names):\n",
        "    yaml_content += f\"  {i}: {class_name}\\n\"\n",
        "\n",
        "config_path = pathlib.Path(data_dir, 'dataset.yaml')\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(yaml_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77A7tN3GZzuG"
      },
      "source": [
        "After setting up the dataset in the format required, we can then load the pre-trained Yolov8 weights (have chosen yolov8n-cls, but can change the n to {s,m,l,x} if large model required, be wary this will increase both training and subsequent inference time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpFSk2itZzuG"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained YOLOv8 classification model\n",
        "yolo_model = YOLO('yolov8n-cls.pt')\n",
        "\n",
        "# Train the model\n",
        "yolo_model.train(data='datasets', epochs=10, imgsz=128, batch=32)\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = yolo_model.val(data='datasets')\n",
        "\n",
        "# Save the model\n",
        "yolo_model.save('yolo.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYX7BKb8ZzuG"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Below we give examples of how to load the weights we created during training and use them to run on sample images from the dataset to visually see performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYzabIr-ZzuG"
      },
      "outputs": [],
      "source": [
        "test_image_path = \"fashion_product_images/myntradataset/images/18008.jpg\"\n",
        "img = tf.keras.utils.load_img(\n",
        "    test_image_path, target_size=(128, 128)\n",
        ")\n",
        "display(img)\n",
        "\n",
        "### CURRENTLY BROKEN!! Cannot load weights I trained on Colab onto my laptop ###\n",
        "# # Tensorflow\n",
        "# loaded_tf = tf.keras.models.load_model('../weights/basic_tf.keras')\n",
        "\n",
        "# img_array = tf.keras.utils.img_to_array(img)\n",
        "# img_array_batch = tf.expand_dims(img_array, 0)\n",
        "\n",
        "# predictions = loaded_tf.predict(img_array_batch)\n",
        "\n",
        "# score = tf.nn.softmax(predictions[0])\n",
        "# top1_val = tf.argmax(score)\n",
        "# top1_str = metadata['articleType'].cat.categories[int(top1_val)]\n",
        "# print(\"Class: \")\n",
        "# print(top1_str)\n",
        "# print(\"Confidence: \")\n",
        "# print(float(score[top1_val]))\n",
        "# print(\"\")\n",
        "\n",
        "# Yolov8\n",
        "with open(\"datasets/dataset.yaml\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    unproc_classes = text.split(\"names:\\n\")[1].split(\"\\n\")\n",
        "    class_names = [c.split(\": \")[1].strip() for c in unproc_classes if c]\n",
        "\n",
        "loaded_yolo = YOLO(model='../weights/yolo.pt', task=\"classify\")\n",
        "\n",
        "results = loaded_yolo.predict(test_image_path, verbose=False)\n",
        "result, = results\n",
        "\n",
        "top1_val = result.probs.top1\n",
        "top1_str = class_names[int(result.names[int(top1_val)])]\n",
        "print(\"Class: \")\n",
        "print(top1_str)\n",
        "print(\"Confidence: \")\n",
        "print(result.probs.top1conf)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}